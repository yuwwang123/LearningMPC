# LearningMPC
Learning based Model Prodictive Control for online iterative trajectory optimization for F1/10 autonomous racing.

The F1/10 hardware used for experiments:

<p align="center">
  <img src="https://storage.googleapis.com/groundai-web-prod/media/users/user_211599/project_333357/images/introduction/images/Fig_1.png" width="350" title="The F1/10 hardware used for experiments">
</p>

The car is learning to improve its racing policy iteratively online as it completes more and more laps, until the policy converges to a minimum lap time. 

Initial Sample Safe Set is collected using a path following controller. Starting from Lap 3, control policies are generated by learning MPC. Every time a lap is completed, cost-to-go values are updated for all collected samples. Stability and recursive feasibility of MPC is gauranteed by enforcing the terminal state to land within a convex hull of a selected subset of collected samples from previous laps. The MPC manages to minimize the cost-to-go for the terminal state at each run, and is solved with OSQP solver at 20Hz. The sample safe set continues to grow as more and more data is being collected along the way.

The improvement of racing performance over laps is shown here below. The black one is the initial lap with path following controller. The colored ones are the improved trajectories given by LMPC

### Dependencies
1. Install the f1/10 racecar simulator:
  https://github.com/mlab-upenn/f110-fall2019-skeletons/tree/master/racecar_simulator
 
2. Install QSQP Solver: 
   https://osqp.org/docs/get_started/sources.html
   
3. Install the C++ Eigen wrapper for OSQP:
   https://robotology.github.io/osqp-eigen/doxygen/doc/html/index.html
   
### How to Run
Build everything with ```catkin_make```  from the root directory of your catkin workspace

Since the simulator uses a different map as default, you will need to add the track map used in this project to the map folder inside the simulator package. Copy ```levinelobby_track.png``` and ```levinelobby_track.yaml``` to ```racecar_simulator\map```, and modify the path in simulator.launch to ```levinelobby_track.yaml```

Then launch the simulator by entering the command below in terminal:

    roslaunch racecar_simulator simulator.launch
    
Position the car approximately in the starting position as shown below using the interactive marker in Rviz (The car must be placed ahead of the starting line)

<img src="media/starting_config.png" width="40%">


Then launch LearningMPC node:

    roslaunch LearningMPC lmpc.launch
    
Add the "LMPC" topic in the Rviz display panel to visualize predicted trajectories by LearningMPC

Enter ```n``` in the simulator terminal to unlock the car

### Note on how to load this map
The map data which consists of an image file and a yaml file with meta info resides in the /data folder. Place the two files under the /map folder in the [F1/10 racecar simulator package](https://github.com/mlab-upenn/f110-fall2019-skeletons/tree/master/racecar_simulator/maps). Finally, modify [this line](https://github.com/mlab-upenn/f110-fall2019-skeletons/blob/master/racecar_simulator/launch/simulator.launch#L7) in the launch file to point to the yaml file (levinelobby_track.yaml in this case) before you launch the simulator. 



### Results

Lap 5 (Top speed 1.2 m/s)

![](media/lap5.gif)

Lap 25 (Top speed 3 m/s)

![](media/lap25.gif)

After Lap 60, it converges to an optimal policy (Top speed 6 m/s) with precisely controlled high speed drifting.

![](media/lap60_converged_drifting.gif).

With low frictions, it converges to a policy with lower top speed but with more drifting.

![](media/drifting_low_friction.gif).

Obstacle Avoidance 1:

![](media/obstacle_avoid.gif).

Obstacle Avoidance 2 (low friction): 

![](media/obstacle_low_friction.gif).

Lap time vs number of iterations completed:

<img src="media/lap_time_decrease.PNG" width="50%">


Converged optimal policy recorded:

<img src="media/final_traj.PNG" width="50%">



Currently working on Gaussian Process and local linear regression for system identification to update the dynamic model online.

The project is inspired by "Learning How to Autonomously Race a Car: a Predictive Control Approach, Ugo Rosolia and Francesco Borrelli"





